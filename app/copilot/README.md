# Banking Assistant Copilot Backend

A FastAPI-based multi-agent orchestration service that powers the banking assistant frontend. This microservice uses Azure OpenAI and the Agent Framework to provide intelligent banking support through specialized agents.

## ğŸ—ï¸ Architecture

This backend implements a **supervisor agent pattern** where:
- **Supervisor Agent**: Routes user requests to specialized domain agents
- **Account Agent**: Handles account balance, payment methods, and beneficiaries
- **Transaction Agent**: Manages banking movements and payment history
- **Payment Agent**: Processes payment requests and bill uploads

## ğŸš€ Quick Start

### Prerequisites

- Python 3.11 or higher
- Azure OpenAI account with API access
- Git with LFS support

### Backend Setup

#### 1. Navigate to the copilot directory

```powershell
cd app/copilot
```

#### 2. Configure Git LFS (if needed)

```powershell
$env:GIT_LFS_SKIP_SMUDGE="1"
```

#### 3. Install dependencies using uv

```powershell
# Install uv if you don't have it
pip install uv

# Create a virtual environment
uv venv

# Activate the virtual environment
.\.venv\Scripts\Activate.ps1

# Install all dependencies
uv sync --active --prerelease=allow 
```

#### 5. Configure environment variables

Update the `.env.dev` file with your Azure OpenAI connection details:

```env
# Azure OpenAI Settings
AZURE_OPENAI_ENDPOINT=https://your-endpoint.openai.azure.com/
AZURE_OPENAI_CHAT_DEPLOYMENT_NAME=gpt-4.1

# Azure services (if needed)
AZURE_DOCUMENT_INTELLIGENCE_SERVICE=your-doc-intel-service
AZURE_STORAGE_ACCOUNT=your-storage-account

# MCP Servers (if running)
ACCOUNT_MCP_URL=http://localhost:8070
TRANSACTION_MCP_URL=http://localhost:8071
PAYMENT_MCP_URL=http://localhost:8072
```

#### 6. Run the development server

**Option A: Using uvicorn directly**
```powershell
# Set PROFILE env variable to "dev". This will make the app load .env.dev file instead of .env.
$env:PROFILE="dev"
uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
```

**Option B: Using VS Code debugger**
1. Navigate to **Run & Debug** in VS Code
2. Select **"FastAPI: DEV Debug Copilot App"** from the dropdown
3. Press F5 or click the green play button

The API will be available at:
- **API**: http://localhost:8000

---

## ğŸ¨ Frontend Setup

### 1. Navigate to the frontend directory

```powershell
cd app/frontend
```

### 2. Install dependencies

```powershell
npm install
```

### 3. Start the development server

```powershell
npm run dev
```
The frontend will be available at:
- **Frontend**: http://localhost:8081

---

## ğŸ“ Project Structure

```
app/copilot/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py                 # FastAPI application entry point
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ chat_routers.py     # Chat endpoints with streaming support
â”‚   â”‚   â””â”€â”€ content_routers.py  # File upload/download endpoints
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ azure_chat/
â”‚   â”‚   â”‚   â”œâ”€â”€ supervisor_agent.py      # Main routing agent
â”‚   â”‚   â”‚   â”œâ”€â”€ account_agent.py         # Account management
â”‚   â”‚   â”‚   â”œâ”€â”€ transaction_agent.py     # Transaction history
â”‚   â”‚   â”‚   â””â”€â”€ payment_agent.py         # Payment processing
â”‚   â”‚   â””â”€â”€ foundry/            # Alternative foundry-based agents
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ container_azure_chat.py      # DI container
â”‚   â”‚   â””â”€â”€ observability.py             # Logging & monitoring
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ chat.py             # Pydantic models
â”‚   â””â”€â”€ helpers/
â”‚       â””â”€â”€ utils.py            # Utility functions
â”œâ”€â”€ pyproject.toml              # Project dependencies
â”œâ”€â”€ uv.lock                     # Lock file for reproducible builds
â””â”€â”€ .env.dev                    # Environment configuration
```

---

## ğŸŒŠ Streaming Support

The backend supports **real-time streaming** responses for a better user experience:

- Enable streaming via the settings panel in the UI
- Toggle "Stream chat completion responses" checkbox
- Responses appear word-by-word in real-time

---
